<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Publication, Research & Writing Samples | Qixin Deng</title>
  <link rel="stylesheet" href="css/style.css" />
</head>
<body>
     <!-- Navigation bar -->
     <nav class="site-nav">
        <a href="index.html">🏠 Home</a>
        <a href="research.html">📄 Research and Publication</a>
        <a href="software.html">🖥️ Software</a>
        <a href="recording.html">🎛️ Recording</a>
        <a href="sound-design.html">🎨 Sound Design</a>
      </nav>
    

<div class="container research-page">

  <main class="main-content">

    <h1>📄 Publication, Research & Writing Samples</h1>
    <p class="page-description">
        A collection of my published papers, academic writing, technical reports, and ongoing research contributions. 
      </p>
      

    <!-- ComposerX -->
    <section class="card">
      <h2> ComposerX</h2>
      <p>
        ComposerX is a multi-agent-based text-to-music generation system powered by GPT where each GPT plays a role in the process of music composition.
        This work has been accepted by ISMIR 2024.
      </p>
      <ul class="resource-links">
        <li><a href="https://arxiv.org/abs/2404.18081" target="_blank">🔗 arXiv Paper</a></li>
        <li><a href="https://lllindsey0615.github.io/ComposerX_demo/" target="_blank">🎧 Demo Page</a></li>
        <li><a href="https://github.com/lllindsey0615/ComposerX" target="_blank">💻 GitHub</a></li>
      </ul>
    </section>

    <!-- Timbre -->
    <section class="card">
      <h2>Exploring Timbre Semantics in Joint Language–Audio Embeddings</h2>
      <p>
        This research examines whether multimodal models such as MS-CLAP, LAION-CLAP, and MuQ-MuLan 
        can capture perceptual dimensions of timbre qualities like brightness, roughness, and warmth 
        that shape how humans perceive sound. Our evaluation shows that LAION-CLAP consistently 
        aligns most closely with human timbre perception across both instrumental sounds and audio effects.
      </p>
      <p><em>This paper is currently under review for ICASSP 2026.</em></p>
      <ul class="resource-links">
        <li><span>📄 Paper: <em>Coming Soon</em></span></li>
        <li><span>💻 Code : <em>Coming Soon</em></span></li>
      </ul>
    </section>
    


    <!-- Survey & Reports -->
    <section class="card">
      <h2>Survey & Technical Writing</h2>
      <ul class="resource-links">
        <li><a href="https://drive.google.com/file/d/1CQgao3g79pbxLbYizwCcib8tL28tF_b7/view?usp=drive_link" target="_blank">📑 Survey on Audio-Visual Learning</a></li>
        <li><a href="https://drive.google.com/file/d/1SclE-KvlVghj3dPxGLLcYyEVCO72zzTz/view?usp=drive_link" target="_blank">📑 Survey on Patten Recognition in Timbre</a></li>
        <li><a href="https://drive.google.com/file/d/1sZovBobkrnBu30eyuQu6Hd-xspEGQETL/view?usp=drive_link" target="_blank">📑 Report: Two-Way Bookshelf Loudspeaker</a></li>
        <li><a href="https://drive.google.com/file/d/15K2k3Y0F-KnvSRIeVCuk5jP-S1awE08D/view?usp=drive_link" target="_blank">📑 Report: WheelTalk Project</a></li>
      </ul>
    </section>

  </main>
</div>

<footer>
  <p>© 2025 Qixin "Lindsey" Deng</p>
</footer>

</body>
</html>
